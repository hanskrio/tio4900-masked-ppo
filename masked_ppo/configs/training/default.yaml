total_timesteps: 20_000  # GLOBAL timesteps for the entire run, train_distributed.py already divides this by world_size.
policy: 'MlpPolicy'
gamma: 0.99
learning_rate: 0.0003
verbose: 1
seed: 42
policy_kwargs: # Arguments passed to the policy constructor
  net_arch: [64, 64]